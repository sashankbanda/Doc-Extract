import json
import logging
import os
from typing import Any, Dict, List, Optional, Union

import litellm

from backend.config import config
from backend.services.semantic_tagger import semantic_tagger
from backend.services.key_manager import key_manager
from backend.services.juror_service import juror_service


logger = logging.getLogger(__name__)

LineMeta = Union[List[Any], Dict[str, Any]]

# Mapping dictionary: canonical_name -> list of synonyms
CANONICAL_MAPPINGS: Dict[str, List[str]] = {
    "lob": [
        "Line of Business", "Line of Insurance", "LOB", "Coverage Code", 
        "Policy Type", "Claim type", "Loss Type", "File Type"
    ],
    "insured": [
        "Insured", "Insured Name", "Insured(s)", "Account Name", "Assured Name", 
        "Named Insured", "Policy Holder", "Location", "Insured Group", "Policy name", 
        "Client/Account", "Client", "Consumer", "Client Name"
    ],
    "dba": [
        "Facility", "Insured Facility", "Insured DBA", "Account Name", 
        "Location Address", "Insured Name", "Org", "Org1"
    ],
    "policyNumber": [
        "Policy Number", "Policy No", "Policy Reference", "Policy #", "Policy Num", 
        "Policy", "Insured Policy Num", "Certificate Num", "Policy-Asco-Mod",
        "Contract Number", "POLICY INQUIRY", "Policy Nbr", "Lead Policy", "Policy Identifier"
    ],
    "effdate": [
        "Policy Effective", "Policy Term", "Cov Eff Date", "Term", "Inceptiion Date", 
        "Period", "Eff Date", "Policy year", "Policy Effective Date Range", "UW Year", 
        "Date(s) Insured", "Contract Period", "Date Contract Effective", 
        "Contract Effective Date", "POL DATE", "Incept", "Coverage Dates", 
        "Term Totals for", "Original Risk Inception Date", "Policy Period Desc", 
        "Losses from", "Cov Eff Date"
    ],
    "expdate": [
        "Policy Expiry", "Policy Term", "Cov Exp Date", "Term", "Exp Date", "Period", 
        "Policy Expiration Date", "Expiry Date", "Period", "Policy Effective Date Range", 
        "Policy Period", "Policy Termination Date", "Policy End Date", "Cov Exp Date"
    ],
    "carrier": [
        "Carrier", "Insurer Name", "Insurance Company", "Writing company", "Broker Name"
    ],
    "valuedDate": [
        "Data", "Valued", "Financial", "Report date", "Valuation Date", "Run Date", 
        "Generated Date", "Data reported", "Loss Run", "Report Executed", 
        "Evaluation Date", "Selected Source Process Date", "Losses Last Updated",
        "Loss Run External", "Transaction Date", "PolicyValuedDate", 
        "Report Date / Time", "Generated by Looker on", "Report Generated", "Prepared", 
        "Report Run Date", "Print date", "Date Produced", "Report print date", "Created", 
        "REPORT RUN"
    ],
    "claimNumber": [
        "Claim No", "Claim", "Claim #", "Claim Reference #", "Claim Num", 
        "Claim Master Reference", "Claim occurrence", "Case #", "Casefile",
        "Claim Number", "Claim Nbr", "File Number", "File #", "Loss Number", 
        "Reference Number", "Matter Number", "Our Reference"
    ],
    "claimant": [
        "Claimant", "Claimant Name", "Claimants", "Injured Employee", "Worker's Name", 
        "Payee", "Employee", "Clmt Name", "Drvr/Clmt Name", 
        "CLAIMANT/DRIVER/LOCATION OF PROPERTY", "Party Name", "IW Name", "PATIENT NAME"
    ],
    "claimStatus": [
        "Status", "Claim Status", "Sts", "Loss Status", "Open or Closed", "O/C", 
        "Clm Stat Desc", "Feature Status", "Open/Closed Flag", "s"
    ],
    "closedDate": [
        "Claim Closed Date", "Closed Date", "Date Closed", "Closed Dt", "ClosedDate",
        "Date Clm Closed", "Close", "Month Closed", "Last Closed Date", "LL Close Date", 
        "Close Date / Reopened Date"
    ],
    "reportedDate": [
        "Claim Date", "Claim Report Date", "Claim Made Date", "Date Received", 
        "Date Rcvd", "Report", "Report Date", "Date Reported", "Reported Date",
        "Date Rep to Insured", "Administrative Notified Date", "Date Incident Reported", 
        "Run Date", "Claim Reported Date", "Date Reported to Carrier", 
        "Reported to Program", "Date Rptd", "Claimant Report Date", "Notice Date", 
        "Notification Date"
    ],
    "dateOfLoss": [
        "Loss Date", "Accident Date", "Injury Date", "Event Date", "Incident Date", 
        "DOL", "DCM", "Inj. Date", "Claim Date", "Notice of Loss", "D/L", 
        "Occurrence Date", "Acc Date", "Loss Incurred Date"
    ],
    "lossDescription": [
        "Desc", "Description", "Loss Description", "Incident / Accident Description", 
        "Injury Description", "Event Description", "Event Detail", "Claim Description", 
        "Incident Description", "Cause Cd Desc", "Loss type", "Injury Detail", 
        "Injury Source", "Cause Of Loss", "ClaimLossNarrative", "Facts", 
        "Case Description", "Loss Reason", "ASL Desc"
    ],
    "lossLocation": [
        "Loss Location", "Location/MD", "Location Name", "Location Address",
        "ACCIDENT LOCATION", "Accident State", "Location/Site", "Loss Loc", 
        "Location Level", "Incident Location"
    ],
    "state": [
        "ST", "State", "Claim Location", "Loss State", "Jurisdiction", "Venue State", 
        "Accident State", "LossStateCd", "State/Province of Loss", "Loss Province", 
        "Loss Loc-State", "Incident State"
    ],
    "city": [
        "Loss City", "City", "Loss Loc-City", "Plant Div"
    ],
    "medicalPaid": [
        "Medical Paid", "Paid Medical", "Med Pd", "Medical Payments", "Pd Med", 
        "MED PAY", "Gross Medical Paid", "Medical/BI (USD) Paid", "Paid-To-Date MED", 
        "Medical PTD", "MTD Payments", "MedicalPayments", "MED Paid", "MED. PAID", "Paid"
    ],
    "medicalReserves": [
        "Medical Reserves", "Medical (O/S Reserves)", "O/S Medical", "Med Res", 
        "Medical Outstanding", "MedicalReserves", "Fut Res Med", "Outstanding Med", 
        "Open Medical Reserves", "Medical/BI (USD) Outstanding", "Medical O/R", 
        "Medical Outstanding Rsvs", "Med O/S Rsvs", "Medical O", 
        "Reserve Amount (Medical)", "Avail Medical", "Medical Res", "Outstanding Med", 
        "Open Medical Reserves", "Medical Reserve", "MED Reserve", "Outstanding1", 
        "MED. RESV", "Reserve"
    ],
    "indemnityPaid": [
        "Indemnity Paid", "Indemnity (Paid to Date)", "Paid Indemnity", 
        "Indem Payments", "Paid Loss", "Loss Paid", "Total Loss Paid", "Paid Ind", 
        "Loss PTD", "CMP Paid", "Comp/PD (USD) Paid", "ITD Indemnity Payment", 
        "Paid-To-Date IND", "Settlement Paid", "TOTAL FOR CLAIM Indemnity Paid", 
        "Loss Paid (FGU)", "Comp PTD", "WKDIS PAID", "ITD Payments", "Loss/ Ind (PD)", 
        "Indemnity(Net Payments)", "Paid Indemnity BI/Med", "Ind Pd", "IndemnityPayments", 
        "Paid Ind", "Gross Indemnity Paid", "Gross Paid Loss", "Payments-Indemnity Paid", 
        "Comp Paid", "Compensation Paid", "Loss Paid To Date", "PAID LOSSES", 
        "Indem Paid", "Paid"
    ],
    "indemnityReserves": [
        "Indemnity Reserves", "Indemnity (O/S Reserves)", "O/S Indemnity", 
        "Indemnity Outstanding", "Indem Reserves", "Fut Res Ind", 
        "Outstanding Reserves Loss", "Loss Reserve", "Total Loss Reserve", 
        "RESERVE LOSSES", "O/S Loss Rsv", "IND Reserve", "CMP Reserve", 
        "Comp/PD (USD) Outstanding", "Case Loss Reserve", 
        "ITD Indemnity Reserve Balance", "OPEN LOSSES", "Reserve IND", 
        "Loss Reserved (FGU)", "Comp Outstanding Rsv", "Compensation Reserve", 
        "WKDIS RESV", "Avail Indemnity", "Ind Out", "Indemnity Res", "Ind Res", 
        "IndemnityReserves", "Outstanding Ind", "Open Indemnity Reserves", 
        "Indemnity Reserve", "Pending Ind", "CMP Reserve", "Outstanding", 
        "Reserves-Indemnity", "Indemnity O/R", "Comp Reserve", "Loss Reserve Amt", 
        "Indem Reserve", "Reserves Indemnity"
    ],
    "expensesPaid": [
        "Expenses Paid", "Expense Paid", "Paid Legal/Expense", "Exp Pd", 
        "Gross Defense Paid", "Defence (Paid)", "Loss Adj. Expense Paid", 
        "LAE Payments", "Pd Exp/Oth", "Defense PTD", "Paid ALAE", "Expense PTD", 
        "ALE Paid", "Alloc Exp Paid", "Defense Paid (FGU)", "Expenses (Paid)", "PD Exp",
        "ExpensePayments", "Paid Expense", "Gross Paid Expense", "Paid LAE", 
        "Total Expense Paid", "ALAE Paid", "Payments-Other Expenses", 
        "Expense Paid To Date", "ALE Paid"
    ],
    "expensesReserves": [
        "Expenses Reserves", "ALAE Reserves", "Outstanding Expense", 
        "O/S Legal/Expense", "Expense Outstanding", "Defence(Outstanding)", 
        "Loss Adj. Expense Outstanding", "LAE Reserves", "Expense Reserve", 
        "Fut Res Exp", "LAE Reserves", "O/S Defense Reserve", 
        "Outstanding Expense Reserves", "ALE Reserve", "O/S Exp Reserve", 
        "Expenses (USD) Outstanding", "Case ALAE Reserve", "Expense Res", "Exp Res", 
        "ExpenseReserves", "Outstanding Exp", "Expense Reserve", "Pending LAE", 
        "ALAE Reserve", "Reserves-Other Expenses", "Expense O/R", 
        "Expense Outstanding", "EXPENSE RESV", "Avail Legal", "Expense Reserve Amt", 
        "OPEN EXPENSE", "Reserves Expense"
    ],
    "totalPaid": [
        "Total Paid", "Paid", "Tot Paid", "Total Payments", "Gross Paid to Date", 
        "Total Claims Paid", "Paid Amount"
    ],
    "totalReserve": [
        "Total Reserve", "Reserves", "Reserved", "Outstanding reserve", 
        "Total Outstanding", "Gross Outstanding", "Gross Reserve", 
        "Remaining reserve", "Reserve (Total Incurred Details)", "Total Open", 
        "O/S Reserve", "Total Claims Outstanding", "Total Out", "Total Res", 
        "Total Outstanding Reserves", "Reserve Amount"
    ],
    "totalIndemnity": [
        "Total Indemnity", "Pd Ind", "Incurred Ind", "INDEMNITY", "COMP INC", 
        "Total Loss paid", "Indemnity Paid"
    ],
    "totalExpenses": [
        "Total Expenses", "Expense", "Pd Exp/Oth", "LAE Incurred", "Net Expense", 
        "Total Expense Paid", "Total Defense", "Total Expense", "Net Expense", 
        "Defense Paid"
    ],
    "totalIncurredSource": [
        "Total Incurred", "Total Incurred Source", "Total (Total Incurred)", 
        "Total Inc", "Total", "Incurred (Total)", "Grand Total Incurred", "Incurred", 
        "TOTAL INCURRED AMOUNT", "TOTALS Incurred", "Gross Incurred", 
        "Incurred Total(Gross)", "TOTAL CLAIM", "Amount Paid", "Totals(USD) Incurred", 
        "Total Case Incurred Before Recoveries", "Incur Total", "Total Incr", 
        "Case Incurred Loss", "Paid Loss", "TotalPaid", "PaidTotal", 
        "Tot (Gross incurred)", "Total I", "Casefile Totals (Incurred)", 
        "Total Claims Incurred", "Total Incurred Amount", "Net Total Inc", 
        "Total Incurred excluding Recovery (Total)", "Claim Total Incurred", "Paid"
    ],
    "recoveries": [
        "Recoveries", "Total Recoveries", "TotalRecoveries", "Tot Recov", "Recovery", 
        "Recovered", "Recovery Incurred", "Incurred Recovery", 
        "Recoveries Impacting Incurred (Incurred)", "Recoveries Impacting Incurred", 
        "Recovery (USD) Incurred", "Deductible  / Recovery", "TotRcvry", 
        "Deductible Recovered", "Applied Recovery", "Tot (Recoveries)", 
        "Paid Deductible", "RECOVERED", "Subro Salvage Recovered", "ITD Recoveries", 
        "Recovery/Refund (Total)", "Inc. Recovery", "Total Recov", "Total Recovery", 
        "Reserves-Recovery Reserves", "Payments-Recovery Received", "Recovery Total", 
        "RECOVERY PENDING"
    ],
}

# Reverse mapping: synonym -> canonical_name (for fast lookup)
SYNONYM_TO_CANONICAL: Dict[str, str] = {}
for canonical_name, synonyms in CANONICAL_MAPPINGS.items():
    for synonym in synonyms:
        # Normalize synonym for matching (case-insensitive, strip whitespace)
        normalized = synonym.strip().lower()
        SYNONYM_TO_CANONICAL[normalized] = canonical_name


def find_canonical_name(source_key: str) -> Optional[str]:
    """
    Find the canonical name for a given source key by matching against synonyms.
    
    Args:
        source_key: The original key as it appears in the document
        
    Returns:
        Canonical name if a match is found, None otherwise
    """
    if not source_key or source_key == "(no key)":
        return None
    
    # Normalize the source key for matching
    normalized = source_key.strip().lower()
    
    # Try exact match first
    if normalized in SYNONYM_TO_CANONICAL:
        return SYNONYM_TO_CANONICAL[normalized]
    
    # Try substring matching (check if any synonym is contained in the key or vice versa)
    for synonym, canonical in SYNONYM_TO_CANONICAL.items():
        if synonym in normalized or normalized in synonym:
            return canonical
    
    return None


class LLMService:
    """
    Handles LLM-powered extraction of raw fields from documents.
    
    The LLM only extracts raw fields with line numbers. All normalization,
    grouping, and highlight generation is handled by backend services.
    """

    def __init__(self) -> None:
        # Default model
        self.default_model = "groq/llama-3.3-70b-versatile"
        self.system_prompt = self._build_system_prompt()
        
    def _get_api_key_for_model(self, model: str) -> Optional[str]:
        """
        Determine which API key to use based on the model prefix/provider.
        """
        # Mapping from model prefix to key provider name in key_manager
        # Providers in key_manager: "groq", "openai", "anthropic", "google", "huggingface", "deepseek", "mistral", "xai"
        
        if model.startswith("groq/"):
            return key_manager.get_key("groq") or config.GROQ_API_KEY
        elif model.startswith("gemini/") or "gemini" in model:
            return key_manager.get_key("google")
        elif model.startswith("gpt-"):
            return key_manager.get_key("openai")
        elif model.startswith("claude-"):
            return key_manager.get_key("anthropic")
        elif model.startswith("huggingface/") or model.startswith("hf/"):
            return key_manager.get_key("huggingface")
        elif "deepseek" in model:
            return key_manager.get_key("deepseek")
        elif "mistral" in model:
             return key_manager.get_key("mistral")
        elif "xai" in model or "grok" in model: # xAI uses 'grok' usually but might be custom endpoint
             return key_manager.get_key("xai")
        
        return None

    def _build_system_prompt(self) -> str:
        """
        Build system prompt that instructs LLM to extract a flat array of items with line numbers.
        No normalization, grouping, structure inference, or coordinate math - just raw extraction.
        """
        # Build mapping synonyms section for the prompt
        mapping_section = "**Field Mapping Synonyms (for reference):**\n\n"
        mapping_section += "The following mappings help identify canonical field names. Use the exact source key as it appears in the document.\n\n"
        for canonical_name, synonyms in CANONICAL_MAPPINGS.items():
            mapping_section += f"- `{canonical_name}`: {', '.join(synonyms[:5])}"  # Show first 5 synonyms
            if len(synonyms) > 5:
                mapping_section += f", ... (and {len(synonyms) - 5} more)"
            mapping_section += "\n"
        mapping_section += "\n"
        
        return (
            "You are an expert insurance document extraction AI. Your goal is to extract **ALL** visible data from the Loss Run Report.\n\n"
            "**CRITICAL: Your role is ONLY to extract raw fields exactly as seen in the document.**\n\n"
            "**STRICT RULES - YOU MUST FOLLOW THESE:**\n\n"
            "1. Extract EVERY visible field/value from the document\n"
            "2. Do NOT normalize keys (use exact labels as they appear)\n"
            "3. Do NOT group claims or create nested structure\n"
            "4. Do NOT invent structure or infer relationships\n"
            "5. Do NOT skip columns or summarize data\n"
            "6. Do NOT infer missing values\n"
            "7. Do NOT guess line numbers - if unclear, skip the item\n\n"
            + mapping_section +
            "**Line Number References:**\n\n"
            "- The raw text contains line numbers in square brackets (e.g., [15], [0x11])\n"
            "- For each field, list ALL line numbers where that field's value appears\n"
            "- Line numbers must match the [NN] markers in the text EXACTLY\n"
            "- Multi-line values → include all line numbers (e.g., [15, 16, 17])\n"
            "- If line numbers are unclear or missing → skip the item (do NOT guess)\n\n"
            "**Output JSON Structure (MANDATORY):**\n\n"
            "Return valid JSON with this EXACT structure:\n"
            "{\n"
            '  "items": [\n'
            "    {\n"
            '      "source_key": "Claimant Name",\n'
            '      "value": "SYDIA",\n'
            '      "line_numbers": [15, 16, 17]\n'
            "    },\n"
            "    {\n"
            '      "source_key": "Claim Number",\n'
            '      "value": "12345",\n'
            '      "line_numbers": [12]\n'
            "    }\n"
            "  ]\n"
            "}\n\n"
            "**Requirements:**\n\n"
            "- `items` is an array\n"
            "- One object per extracted value\n"
            "- `source_key` = exact label as it appears in document (use the original field name as seen)\n"
            "- `value` = exact value as it appears (no transformation)\n"
            "- `line_numbers` = array of integers matching [NN] markers exactly\n"
            "- If a value exists, it MUST have line_numbers\n"
            "- If line_numbers are unclear → skip the item (do NOT guess)\n"
            "- Keep strictly to JSON. Do not add comments or extra keys.\n"
            "- If a field is not present, omit it (do not include null values)."
        )

    async def structure_document(
        self, 
        raw_text: str, 
        line_metadata: Union[List[LineMeta], Dict[str, LineMeta]],
        model_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Extract flat items array from document using LLM.
        
        Args:
            raw_text: Content to extract from
            line_metadata: Metadata for lines (used if we need to look up coords)
            model_id: Optional specific model to use (e.g., 'gemini/gemini-pro', 'groq/llama3-8b')
        """
        target_model = model_id if model_id else self.default_model
        api_key = self._get_api_key_for_model(target_model)
        
        logger.info(f"[LLMService] Using model: {target_model}")
        
        # Call LLM to extract items
        messages = [
            {"role": "system", "content": self.system_prompt},
            {
                "role": "user",
                "content": (
                    "Extract data from the following document text. "
                    "Text includes line numbers in square brackets like [12] or [0x11]. "
                    "List the line numbers for each field in the 'line_numbers' array.\n\n"
                    f"{raw_text}"
                ),
            },
        ]

        # Prepare kwargs for litellm
        kwargs = {
            "model": target_model,
            "messages": messages,
            "response_format": {"type": "json_object"},
        }
        
        if api_key:
            kwargs["api_key"] = api_key
        
        try:
            response = await litellm.acompletion(**kwargs)
            content = response["choices"][0]["message"]["content"]
        except Exception as e:
            logger.error(f"LLM Call Failed for model {target_model}: {e}")
            raise

        try:
            parsed = json.loads(content)
        except json.JSONDecodeError:
            # If the model returns leading/trailing text, try to salvage JSON payload.
            start = content.find("{")
            end = content.rfind("}")
            if start != -1 and end != -1 and end > start:
                parsed = json.loads(content[start : end + 1])
            else:
                raise

        # Extract items from LLM response
        raw_items = parsed.get("items", [])
        if not raw_items:
            logger.warning("[LLMService] No items found in LLM response")
            raw_items = []

        # Convert and validate items: normalize line_numbers format
        items = []
        
        for item in raw_items:
            # Support both "key" (old format) and "source_key" (new format) for backward compatibility
            source_key = item.get("source_key", item.get("key", "")).strip()
            value = item.get("value", "")
            if value is not None:
                value = str(value).strip()
            else:
                value = ""
            line_numbers_raw = item.get("line_numbers", [])
            
            # PRESERVE ALL ITEMS - even with empty keys/values (data loss prevention)
            # Use empty string for missing keys/values instead of skipping
            
            # Convert line numbers to integers (handles hex, strings, etc.)
            line_numbers = []
            for line_val in line_numbers_raw:
                converted = self._convert_line_number(line_val)
                if converted is not None:
                    line_numbers.append(converted)
            
            # If no valid line numbers, use empty array (preserve item, just no line refs)
            # This ensures no data loss - item is still included
            
            # Find canonical name from mapping
            canonical_name = find_canonical_name(source_key) if source_key else None
            
            # Add item (preserved even if key/value/line_numbers are empty)
            items.append({
                "source_key": source_key if source_key else "(no key)",
                "canonical_name": canonical_name,
                "value": value if value else "(no value)",
                "line_numbers": sorted(list(set(line_numbers))) if line_numbers else []  # Deduplicate and sort
            })
        
        # No items are skipped - all items are preserved (data loss prevention)
        logger.info(f"[LLMService] Extracted {len(items)} items (all preserved, no data loss)")
        
        # Add semantic_type tags to all items (deterministic, dictionary-based)
        tagged_items = semantic_tagger.tag_items(items)
        
        # Return flat, lossless structure - no grouping, no claims, no sections
        # Return flat, lossless structure - no grouping, no claims, no sections
        # NEW: Run Juror Verification for High-Risk Fields (Description/Notes)
        # We do this asynchronously/parallel if possible, or sequential for safety first.
        # Since we are inside an async function, we can await checks.
        
        # Prepare context cache to avoid re-fetching raw text slice repeatedly? 
        # Actually raw_text is in memory. We need to map line numbers to text.
        
        # Parse raw_text into lines for fast context lookup
        raw_lines = raw_text.splitlines()
        
        verified_items = []
        for item in tagged_items:
            # Check if this item needs verification
            semantic_type = item.get("semantic_type", "")
            
            # Target Descriptions and Notes (most prone to truncation)
            if semantic_type in ["claim.description", "claim.notes", "claim.cause_of_loss"]:
                # Get Context
                line_nums = item.get("line_numbers", [])
                context_text = self._get_context_for_items(line_nums, raw_lines)
                
                if context_text:
                    # Run Juror
                    # Note: For production speed, we should gather all these promises and await_all
                    # For now, sequential is safer to implement/debug
                    verification = await juror_service.verify_item_completeness(
                        item.get("source_key", ""),
                        item.get("value", ""),
                        context_text,
                        model_id=None # Use default fast juror
                    )
                    item["verification_status"] = verification
                else:
                    item["verification_status"] = {"status": "unverified", "reason": "No context found"}
            else:
                 # Default verified for other fields (dates, amounts usually okay or strict regex)
                 item["verification_status"] = {"status": "verified", "reason": "Low-risk field"}
            
            verified_items.append(item)

        return {
            "items": verified_items
        }

    def _get_context_for_items(self, line_numbers: List[int], raw_lines: List[str], buffer: int = 2) -> str:
        """
        Extract lines from raw_lines around the given line_numbers.
        Handles the [NN] markers to map logical line numbers to array indices.
        
        Note: The raw_text usually contains lines starting with "0xNN: ".
        We need to match the line_numbers (integers) to these prefixes.
        """
        if not line_numbers:
            return ""
            
        # Sort lines
        nums = sorted(line_numbers)
        min_line = nums[0]
        max_line = nums[-1]
        
        context_parts = []
        
        # Simple extraction: iterate input lines and look for markers
        # Optimized: Scan raw lines once or use a map?
        # Given typical file size < 500 lines, iteration is fine.
        
        # We want lines from (min_line - buffer) to (max_line + buffer)
        target_indices = []
        
        # Create a mapping of logical_line_num -> (index, content)
        # raw_lines format: "0x01: text..."
        
        line_map = {}
        for idx, text in enumerate(raw_lines):
            # Try to parse prefix
            # Assuming format "0xNN: " or "[NN] "
            prefix_end = text.find(":")
            if prefix_end != -1:
                prefix = text[:prefix_end].strip()
                # Parse number
                line_num = self._convert_line_number(prefix)
                if line_num is not None:
                    line_map[line_num] = (idx, text)
        
        # Now gather context
        # We want the contiguous block from min-2 to max+2 if they exist in the map
        start_context = max(0, min_line - buffer)
        end_context = max_line + buffer
        
        gathered_lines = []
        # We iterate through the logical range
        for l_num in range(start_context, end_context + 1):
            if l_num in line_map:
                gathered_lines.append(line_map[l_num][1])
                
        return "\n".join(gathered_lines)

    def _convert_line_number(self, line_val: Any) -> Optional[int]:
        """
        Convert a line number to an integer, handling hex strings and other formats.
        
        Handles:
        - Integers: returns as-is
        - Hex strings: "2A" -> 42, "0x2A" -> 42
        - Float integers: 2.0 -> 2
        - String integers: "42" -> 42
        
        Returns None if conversion fails.
        """
        if isinstance(line_val, int):
            return line_val if line_val >= 0 else None
        
        if isinstance(line_val, float):
            # Check if it's effectively an integer
            if line_val.is_integer() and line_val >= 0:
                return int(line_val)
            return None
        
        if isinstance(line_val, str):
            line_str = line_val.strip()
            # Try hex format (with or without 0x prefix)
            if line_str.startswith("0x") or line_str.startswith("0X"):
                try:
                    return int(line_str, 16)
                except ValueError:
                    pass
            # Try hex without prefix (e.g., "2A", "2C")
            try:
                # Check if it looks like hex (contains A-F)
                if any(c in line_str.upper() for c in "ABCDEF"):
                    return int(line_str, 16)
            except ValueError:
                pass
            # Try regular integer
            try:
                val = int(line_str)
                return val if val >= 0 else None
            except ValueError:
                pass
        
        return None



llm_service = LLMService()


